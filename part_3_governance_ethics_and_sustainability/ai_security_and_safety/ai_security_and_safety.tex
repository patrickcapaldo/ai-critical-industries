\chapter{AI Security and Safety}
\label{chap:ai_security_and_safety}

\section{Introduction}
\label{sec:security_introduction}
As Artificial Intelligence systems become increasingly integrated into critical industries, ensuring their security and safety is paramount. AI security focuses on protecting AI systems from malicious attacks and misuse, safeguarding their confidentiality, integrity, and availability. AI safety, on the other hand, broadly concerns preventing unintended negative consequences and ensuring that AI systems operate reliably and align with human values. This chapter will delve into the expanded attack surface introduced by AI, explore scenarios where systems can fail on their own, and outline frameworks for building secure and safe AI systems in high-stakes environments.

\section{The Expanded Attack Surface of AI Systems}
\label{sec:attack_surface}

The integration of AI into various sectors significantly expands the cyber attack surface. A comprehensive list of risks and challenges is available in the Master Risk Register (Appendix \ref{app:master_risk_register}). Key risks related to the expanded attack surface include:

\begin{itemize}
    \item New Components
    \item Increased Complexity and Interconnectivity
    \item Shadow AI
    \item AI-Enhanced Attacks
\end{itemize}

\begin{warning}
\textbf{The Risk of "Shadow AI":} Be particularly vigilant about "Shadow AI"â€”AI tools and systems used by employees without official approval or oversight. While often adopted for productivity gains, these unsanctioned applications can introduce significant security vulnerabilities, data privacy risks, and compliance breaches, as they operate outside of the organization's established security and governance frameworks.
\end{warning}

For a detailed description of each risk and potential mitigation strategies, please refer to the Master Risk Register in Appendix \ref{app:master_risk_register}.

\section{Safety and Reliability: When Systems Fail on Their Own}
\label{sec:safety_and_reliability}

Beyond malicious attacks, AI systems can also fail due to inherent design flaws, biases, or unexpected real-world conditions. A comprehensive list of risks and challenges is available in the Master Risk Register (Appendix \ref{app:master_risk_register}). Key safety and reliability failures include:

\begin{itemize}
    \item Bias and Discrimination
    \item Unintended Harmful Outputs
    \item Over-reliance and Lack of Human Oversight
    \item Brittleness and Lack of Robustness
    \item AI Hallucinations
\end{itemize}

\section{A Framework for AI Security and Safety}
\label{sec:security_framework}
To mitigate these risks, several frameworks have been developed to guide the responsible design, development, and deployment of AI systems:

\begin{itemize}
    \item \textbf{NIST AI Risk Management Framework (AI RMF):} Developed by the U.S. National Institute of Standards and Technology, this voluntary framework provides structured guidance for identifying, assessing, and mitigating risks associated with AI systems. It emphasizes trustworthiness, security, reliability, and resilience \parencite{NIST2023AIRMF}.
    \item \textbf{EU AI Act:} This pioneering legislative framework categorizes AI systems based on their risk level, imposing stricter requirements on high-risk applications. It mandates rigorous risk assessments, transparency obligations, and stringent data governance measures to prevent biases and protect personal data \parencite{EU_AI_Act_2024}.
    \item \textbf{Google's Secure AI Framework (SAIF):} SAIF focuses on integrating security practices throughout the entire lifecycle of AI system development, from design to deployment and maintenance. It aims to ensure that AI models are secure by default \parencite{SafetyGoogle2023}.
    \item \textbf{Core Principles of Trustworthy AI:} Many frameworks and guidelines emphasize foundational principles such as accountability, explainability (understanding AI decisions), fairness (mitigating bias), interpretability and transparency, privacy and security, reliability and robustness (consistent performance), and safety (minimizing harm) \parencite{MaximAI2025}.
\end{itemize}

\section{Leader's Toolkit}
\label{sec:security_leaders_toolkit}

A general toolkit for leaders is available in Appendix \ref{app:leaders_master_toolkit}. For this chapter, a proactive approach to AI security and safety is non-negotiable.

\section{Assessment Questions}
\label{sec:security_assessment_questions}

\section{Further Reading}
\label{sec:security_further_reading}
